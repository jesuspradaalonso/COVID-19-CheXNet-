{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Generic\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Images\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import talos as ta\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import print_summary\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chexNet weights\n",
    "# https://github.com/brucechou1983/CheXNet-Keras\n",
    "chexnet_weights = 'chexnet/best_weights.h5'\n",
    "\n",
    "def chexnet_preprocess_input(value):\n",
    "    return preprocess_input(value)\n",
    "\n",
    "\n",
    "def get_chexnet_model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    base_weights = 'imagenet'\n",
    "\n",
    "    # create the base pre-trained model\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        input_tensor=img_input,\n",
    "        input_shape=input_shape,\n",
    "        weights=base_weights,\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    x = base_model.output\n",
    "    # add a logistic layer -- let's say we have 14 classes\n",
    "    predictions = Dense(\n",
    "        14,\n",
    "        activation='sigmoid',\n",
    "        name='predictions')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=img_input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # load chexnet weights\n",
    "    model.load_weights(chexnet_weights)\n",
    "\n",
    "    # return model\n",
    "    return base_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### funciones\n",
    "\n",
    "def get_class_weight(csv_file_path, target_class, learning_rate):\n",
    "    df = pd.read_csv(csv_file_path, sep=';')\n",
    "    total_counts = df.shape[0]\n",
    "    class_weight = []\n",
    "\n",
    "    ratio_pos = df.loc[(df[target_class] == 'Y')].shape[0] / total_counts\n",
    "    ratio_neg = df.loc[(df[target_class] == 'N')].shape[0] / total_counts\n",
    "    class_weight = np.array((ratio_pos, ratio_neg))\n",
    "        \n",
    "    return class_weight\n",
    "\n",
    "\n",
    "#def auc(y_true, y_pred):\n",
    "#    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "#    K.get_session().run(tf.local_variables_initializer())\n",
    "#    return auc\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "    return value\n",
    "\n",
    "def get_model(learning_rate):\n",
    "    # get base model, model\n",
    "    base_model, chexnet_model = get_chexnet_model()\n",
    "    # print a model summary\n",
    "    # print_summary(base_model)\n",
    "\n",
    "    x = base_model.output\n",
    "    # Dropout layer\n",
    "    x = Dropout(0.2)(x)\n",
    "    # one more layer (relu)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    x = Dropout(0.2)(x)\n",
    "    # add a logistic layer -- let's say we have 6 classes\n",
    "    predictions = Dense(\n",
    "        1,\n",
    "        activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all base_model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # initiate an Adam optimizer\n",
    "    opt = Adam(\n",
    "        lr=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        decay=0.0,\n",
    "        amsgrad=False\n",
    "    )\n",
    "    \n",
    "    #K.clear_session()\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=[auc])\n",
    "    #K.clear_session()\n",
    "    \n",
    "    return base_model, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_talos(X_train, y_train, X_val, y_val, params):\n",
    "    # get base model, model\n",
    "    base_model, chexnet_model = get_chexnet_model()\n",
    "    # print a model summary\n",
    "    # print_summary(base_model)\n",
    "\n",
    "    x = base_model.output\n",
    "    # Dropout layer\n",
    "    #x = Dropout(0.2)(x)\n",
    "    # one more layer (relu)\n",
    "    x = Dense(params['first_neuron'], activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    #x = Dropout(0.2)(x)\n",
    "    #x = Dense(256, activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    #x = Dropout(0.2)(x)\n",
    "    # add a logistic layer -- let's say we have 6 classes\n",
    "    predictions = Dense(\n",
    "        1,\n",
    "        activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all base_model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # initiate an Adam optimizer\n",
    "    opt = Adam(\n",
    "        lr=0.005,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        decay=0.00001,\n",
    "        amsgrad=False\n",
    "    )\n",
    "\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=[auc]\n",
    "        #metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # This callback writes logs for TensorBoard\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir='./Graph', \n",
    "        histogram_freq=0,  \n",
    "        write_graph=True\n",
    "    )\n",
    "\n",
    "    my_callbacks = EarlyStopping(monitor='val_auc', patience=300, verbose=1, mode='max')\n",
    "\n",
    "    callbacks_list = [tensorboard, my_callbacks]\n",
    "\n",
    "\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "    out = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                     validation_data=datagen.flow(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=params['epochs'],\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list,       \n",
    "                     verbose=0)\n",
    "\n",
    "\n",
    "    return out, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_split = [0.8, 0.2]\n",
    "input_shape = (224, 224, 3)\n",
    "batch_size = 16\n",
    "epochs = 10000\n",
    "np.random.seed(23)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case_features = pd.read_csv(os.path.join('../data/clean_data.csv'), sep=';');\n",
    "new_case_features = pd.read_csv(os.path.join('../data/new_clean_data.csv'), sep=';');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_class = 'survival'\n",
    "\n",
    "### Train and val\n",
    "# Selección del patrón de datos X y del target y\n",
    "#y = case_features[target_class]\n",
    "#del case_features[target_class]\n",
    "#X = case_features\n",
    "#Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "### Test\n",
    "#ytest = new_case_features[target_class]\n",
    "#del new_case_features[target_class]\n",
    "#Xtest = new_case_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 'survival'\n",
    "\n",
    "### Train and val\n",
    "# Split patient ids\n",
    "positive = case_features[case_features.survival.values == 'Y']\n",
    "patient_ids = positive.patientid.unique()\n",
    "train_ids = np.random.choice(patient_ids, size = math.floor(perc_split[0]*len(patient_ids)), replace = False)\n",
    "test_ids = patient_ids[~np.isin(patient_ids, train_ids)];\n",
    "\n",
    "negative = case_features[case_features.survival.values == 'N']\n",
    "patient_ids = negative.patientid.unique()\n",
    "train_ids = np.append(train_ids, np.random.choice(patient_ids, size = math.floor(perc_split[0]*len(patient_ids)), replace = False))\n",
    "test_ids = np.append(test_ids, patient_ids[~np.isin(patient_ids, train_ids)]);\n",
    "\n",
    "# Split dataset based on patient ids\n",
    "case_features_train = case_features[case_features.patientid.isin(train_ids)]\n",
    "case_features_val = case_features[case_features.patientid.isin(test_ids)]\n",
    "\n",
    "# Selección del patrón de datos X y del target y\n",
    "ytrain = case_features_train[target_class]\n",
    "del case_features_train[target_class]\n",
    "Xtrain = case_features_train\n",
    "\n",
    "yval = case_features_val[target_class]\n",
    "del case_features_val[target_class]\n",
    "Xval = case_features_val\n",
    "\n",
    "\n",
    "### Test\n",
    "ytest = new_case_features[target_class]\n",
    "del new_case_features[target_class]\n",
    "Xtest = new_case_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(Xtrain.shape[0]):\n",
    "    image_path = os.path.join('../data/', 'images', Xtrain.iloc[i].filename)\n",
    "    imagen = Image.open(image_path)\n",
    "    imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_train.append(imagen)\n",
    "\n",
    "X_train = np.stack(X_train, axis = 0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "for i in range(Xval.shape[0]):\n",
    "    image_path = os.path.join('../data/', 'images', Xval.iloc[i].filename)\n",
    "    imagen = Image.open(image_path)\n",
    "    imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_val.append(imagen)\n",
    "\n",
    "X_val = np.stack(X_val, axis = 0)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(Xtest.shape[0]):\n",
    "    image_path = os.path.join('../data/', 'new_images', Xtest.iloc[i].filename + '.jpeg')\n",
    "    imagen = Image.open(image_path)\n",
    "    imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_test.append(imagen)\n",
    "\n",
    "X_test = np.stack(X_test, axis = 0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain[ytrain=='Y'] = 0\n",
    "ytrain[ytrain=='N'] = 1\n",
    "y_train = np.array(ytrain, dtype = np.int64)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yval[yval=='Y'] = 0\n",
    "yval[yval=='N'] = 1\n",
    "y_val = np.array(yval, dtype = np.int64)\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest[ytest=='Y'] = 0\n",
    "ytest[ytest=='N'] = 1\n",
    "y_test = np.array(ytest, dtype = np.int64)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_val == 0) / len(y_val)\n",
    "ratio_neg = np.count_nonzero(y_val == 1) / len(y_val)\n",
    "class_weight_val = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_test == 0) / len(y_test)\n",
    "ratio_neg = np.count_nonzero(y_test == 1) / len(y_test)\n",
    "class_weight_test = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True, \n",
    "                             rotation_range=90)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = datagen.flow(X_train, y_train, batch_size=100)\n",
    "#Xtr,Ytr = batch[0]\n",
    "#Xtr2 = np.squeeze(Xtr[0,:,:,:])\n",
    "#plt.imshow(Xtr2, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "    os.getcwd(),\n",
    "    '../saved_models'\n",
    ")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# This callback saves the weights of the model after each epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../saved_models/weights.epoch_{epoch:02d}.hdf5',\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./Graph', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard_augmented = TensorBoard(\n",
    "    log_dir='./Graph_augmented', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "my_callbacks = EarlyStopping(monitor='val_auc', patience=200, verbose=1, mode='max')\n",
    "my_callbacks_augmented = EarlyStopping(monitor='val_auc', patience=100, verbose=1, mode='max')\n",
    "\n",
    "\n",
    "callbacks_list = [tensorboard, my_callbacks]\n",
    "callbacks_list_augmented = [tensorboard_augmented, my_callbacks_augmented]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = [2000, 10000]\n",
    "batch_size = [10,15]\n",
    "learning_rate = [0.0001, 0.001, 0.005]\n",
    "\n",
    "epochs = [10000]\n",
    "batch_size = [2**4, 2**5, 2**6]\n",
    "learning_rate = [0.0001, 0.001, 0.01]\n",
    "previous_val_auc = 0\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for e in epochs:\n",
    "    for b in batch_size:\n",
    "        for l in learning_rate:\n",
    "            print(\"Numero de épocas\", e,\", batch size\", b,  \", learning rate\", l)\n",
    "\n",
    "            # Augmented\n",
    "            base_model2, model2 = get_model(l)\n",
    "            ###### ENTRENAMIENTO\n",
    "            out = model2.fit_generator(datagen.flow(X_train, y_train, batch_size=b),\n",
    "                                             validation_data=datagen.flow(X_val, y_val),\n",
    "                                             steps_per_epoch=len(X_train) / b, \n",
    "                                             epochs=e,\n",
    "                                             class_weight=class_weight_train,\n",
    "                                             callbacks = callbacks_list_augmented,       \n",
    "                                             verbose=0)\n",
    "\n",
    "            train_loss = out.history['loss'][-1]\n",
    "            val_loss = out.history['val_loss'][-1]\n",
    "            train_auc = out.history['auc'][-1]\n",
    "            val_auc = out.history['val_auc'][-1]\n",
    "            stopped_epoch = my_callbacks.stopped_epoch\n",
    "            model = out.model\n",
    "            \n",
    "            res = pd.DataFrame([e, l, b, stopped_epoch, train_loss, val_loss, train_auc, val_auc])\n",
    "            df = pd.concat([df, res], axis=1)\n",
    "            #df.index = ['epochs', 'learning_rate', 'batch_size',\n",
    "            #                 'early_stopping', 'train_loss', 'val_loss Cruz', 'train_auc', 'val_auc']\n",
    "            df.to_csv('model_results3.csv') \n",
    "            \n",
    "            if(previous_val_auc < val_auc):\n",
    "                save_dir = os.path.join(\n",
    "                    os.getcwd(),\n",
    "                    '../model_results_y'\n",
    "                )\n",
    "                if not os.path.isdir(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model.save('../model_results_y/model.h5')\n",
    "                \n",
    "                previous_val_auc = val_auc\n",
    "                           \n",
    "df.index = ['epochs', 'learning_rate', 'batch_size',\n",
    "            'early_stopping', 'train_loss', 'val_loss', 'train_auc', 'val_auc']\n",
    "df.to_csv('model_results3.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = ['epochs', 'learning_rate', 'batch_size',\n",
    "            'early_stopping', 'train_loss', 'val_loss', 'train_auc', 'val_auc']\n",
    "df.to_csv('model_results2.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_val = model.predict(X_val)\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "print('AUC train = %s - AUC val = %s - AUC test = %s' % (str(auc_train), str(auc_val), str(auc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_train = (pred_train >= 0.5).astype(int)\n",
    "y_labels_val = (pred_val >= 0.5).astype(int)\n",
    "y_labels_test = (pred_test >= 0.5).astype(int)\n",
    "cm_train = confusion_matrix(y_pred = y_labels_train, y_true = y_train)\n",
    "cm_val = confusion_matrix(y_pred = y_labels_val, y_true = y_val)\n",
    "cm_test = confusion_matrix(y_pred = y_labels_test, y_true = y_test)\n",
    "print(cm_train)\n",
    "print(cm_val)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr_train, tpr_train, threshold_train = roc_curve(y_train, pred_train)\n",
    "roc_auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "fpr_val, tpr_val, threshold_val = roc_curve(y_val, pred_val)\n",
    "roc_auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "fpr_test, tpr_test, threshold_test = roc_curve(y_test, pred_test)\n",
    "roc_auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'r', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.plot(fpr_val, tpr_val, 'g', label = 'AUC = %0.2f' % roc_auc_val)\n",
    "plt.plot(fpr_test, tpr_test, 'b', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'b', label = 'AUC = %0.2f' % roc_auc_train)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.01, 1])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join('../predictions_y', 'X_train.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_train.csv'), pred_train, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_train.csv'), y_train, delimiter=\";\")\n",
    "X_val.to_csv(os.path.join('../predictions_y', 'X_val.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_val.csv'), pred_val, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_val.csv'), y_val, delimiter=\";\")\n",
    "X_test.to_csv(os.path.join('../predictions_y', 'X_test.csv'))\n",
    "np.savetxt(os.path.join('../predictions_y', 'predictions_test.csv'),pred_test, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions_y', 'y_test.csv'), y_test, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TALOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talos as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    'first_neuron': [512],\n",
    "    'epochs' :[20, 100],\n",
    "    'batch_size': [10,15]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ta.Scan(x = X_train, y = y_train, x_val = X_val, y_val = y_val, params = p, model = get_model_talos,\n",
    "            experiment_name ='exp_1', print_params= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ENTRENAMIENTO\n",
    "\n",
    "#history = model.fit(X_train, \n",
    "#          y_train,\n",
    "#          validation_split=0.25,\n",
    "#          shuffle=True,\n",
    "#          batch_size=16, \n",
    "#          nb_epoch=50,\n",
    "#          class_weight=class_weight,       \n",
    "#          verbose=1)\n",
    "\n",
    "history = model.fit(X_train,y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          batch_size=batch_size, \n",
    "          nb_epoch=epochs,\n",
    "          class_weight=class_weight_train,\n",
    "          callbacks = callbacks_list,\n",
    "          verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "    os.getcwd(),\n",
    "    '../model_results_y'\n",
    ")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model.save('../model_results_y/fixed_model.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 3\n",
    "###### ENTRENAMIENTO\n",
    "out = model2.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                     validation_data=datagen.flow(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=epochs,\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list,       \n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('../model_results_y/augmented_model.h5')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
