{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = \"/home/\";\n",
    "path_project = os.path.join(path_root, \"jesusprada\", \"proyecto_python\",\"x-ray\");\n",
    "path_scripts = os.path.join(path_project, \"scripts\");\n",
    "path_data = os.path.join(path_project, \"predictions\");\n",
    "path_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_var = [\"dataset\", \"target\"]\n",
    "min_transactions = 3;\n",
    "max_n_var = 300; # Maximum possible number of variables\n",
    "samples_per_var = 1; # n_variables*samples_per_var <= n_samples;\n",
    "min_n_categories = 5; # Minimum number of categories per variable to set as maximum\n",
    "min_size_others = 3; # Minimun number of low frequency categories to apply grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = pd.read_csv(os.path.join(path_data, 'X_train_augmented.csv'), sep=',');\n",
    "dat_train['pred'] = pd.read_csv(os.path.join(path_data, \"predictions_train_augmented.csv\"), header=None)\n",
    "dat_train['dataset'] = \"train\";\n",
    "dat_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_val = pd.read_csv(os.path.join(path_data, 'X_val_augmented.csv'), sep=',');\n",
    "dat_val['pred'] = pd.read_csv(os.path.join(path_data, \"predictions_val_augmented.csv\"), header=None)\n",
    "dat_val['dataset'] = \"val\";\n",
    "dat_val.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_test = pd.read_csv(os.path.join(path_data, 'X_test_augmented.csv'), sep=',');\n",
    "dat_test['pred'] = pd.read_csv(os.path.join(path_data, \"predictions_test_augmented.csv\"), header=None)\n",
    "dat_test['dataset'] = \"test\";\n",
    "dat_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = dat_train.columns & dat_test.columns\n",
    "dat_train = dat_train[columns_names]\n",
    "dat_val = dat_val[columns_names]\n",
    "dat_test = dat_test[columns_names]\n",
    "\n",
    "dat = pd.concat([dat_train, dat_val, dat_test], axis=0, ignore_index=True)\n",
    "print(dat_train.shape)\n",
    "print(dat_val.shape)\n",
    "print(dat_test.shape)\n",
    "print(dat.shape)\n",
    "dat.groupby([\"dataset\", \"survival\"]).size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select vars\n",
    "target = dat[\"survival\"]\n",
    "selected_vars = [\"dataset\",  \"offset\", \"sex\", \"age\", \"view\", \"location\", \"pred\"]\n",
    "dat = dat[selected_vars]\n",
    "dat[\"target\"] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_vars = dat.columns[dat.nunique() <= 1]\n",
    "dat = dat[dat.columns[dat.nunique() > 1]]\n",
    "#dat.loc[:, (dat != dat.iloc[0]).any()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove not informed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_nas = 0.4\n",
    "index_na = dat.apply(lambda x: x.isna().sum(), axis=0)/dat.shape[0] < threshold_nas\n",
    "\n",
    "dat = dat.iloc[:, index_na.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns[dat.dtypes == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_var = [\"dataset\", \"target\"]\n",
    "categorical_vars = ['sex', 'view', 'location']\n",
    "numerical_vars = list(set(dat.columns) - set(categorical_vars) - set(fixed_var))\n",
    "numerical_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_fill_nan = 'mean'\n",
    "numerical_dat = dat[numerical_vars]\n",
    "categorical_dat = dat[categorical_vars]\n",
    "\n",
    "if(method_fill_nan == 'mean'):\n",
    "    numerical_dat.apply(lambda x: x.fillna(x.mean(), inplace=True), axis=0)\n",
    "    #categorical_dat.apply(lambda x: x.fillna(x.mode()[0], inplace=True), axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos los datos numericos con missing values imputados con los categoricos para tener el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat = pd.concat(categorical_dat, numerical_dat)\n",
    "#dat\n",
    "\n",
    "#datos = pd.concat((numerical_dat, dat[categorical_dat].reset_index()), axis=1)\n",
    "#del datos['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dat\n",
    "#from sklearn.preprocessing import Imputer\n",
    "#imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "#imputer = imputer.fit(X[:, 1:3])\n",
    "#X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dat.nunique()\n",
    "threshold_categories = 10\n",
    "index_gruop_categories = categorical_dat.apply(lambda x: x.nunique(), axis=0)> threshold_categories\n",
    "index_gruop_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dat[\"sex\"].value_counts()\n",
    "categorical_dat.apply(lambda x: x.value_counts()/x.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse = False)\n",
    "categorical_dat = categorical_dat.replace(np.nan,'None')\n",
    "ohe_fit = ohe.fit(categorical_dat)\n",
    "X_ohe = pd.DataFrame(ohe.fit_transform(categorical_dat))\n",
    "X_ohe.columns = pd.DataFrame(ohe_fit.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos los datos categoricos con one hot enconding y los numericos para tener el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.concat((X_ohe, numerical_dat.reset_index()), axis=1)\n",
    "datos = pd.concat((datos, dat[fixed_var].reset_index()), axis=1)\n",
    "del datos['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que ahora todos los datos son numericos menos fecha salida que lo quitaremos por ser una importante del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.columns[datos.dtypes == object]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fecha_salida_values = datos['fecha_salida']\n",
    "y = datos[fixed_var]\n",
    "del datos['target']\n",
    "del datos['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_scale = pd.DataFrame(scale(datos))\n",
    "datos_scale.columns = datos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = datos_scale\n",
    "datos['target'] = y['target']\n",
    "datos['dataset'] =  y['dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[\"dataset\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = datos[datos.dataset=='train']\n",
    "y_train = X_train[\"target\"]\n",
    "del X_train[\"dataset\"]\n",
    "del X_train[\"target\"]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = datos[datos.dataset=='val']\n",
    "y_val = X_val[\"target\"]\n",
    "del X_val[\"dataset\"]\n",
    "del X_val[\"target\"]\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = datos[datos.dataset=='test']\n",
    "y_test = X_test[\"target\"]\n",
    "del X_test[\"dataset\"]\n",
    "del X_test[\"target\"]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score as metric;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1;\n",
    "nthread = multiprocessing.cpu_count() - 1;\n",
    "nthread\n",
    "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a calcular los parametros de la SVM segun Cherskaskky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[0];\n",
    "d = X_train.shape[1];\n",
    "m = np.mean(y_train);\n",
    "s = np.std(y_train);\n",
    "C_cherk = np.max([np.abs(m + 3*s),np.abs(m - 3*s)]);\n",
    "gamma_cherk = np.power(0.2, 1/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresion Logística\n",
    "regularization_values = ['l1', 'l2', 'none'];\n",
    "penalty_values = [1, 10, 100];\n",
    "\n",
    "# SVM\n",
    "C_values = [C_cherk, 5e-03, 4.5e-03, 4e-03];\n",
    "gamma_kernel_values = [gamma_cherk, 3.26e-09, 3.255e-09, 3.25e-09];\n",
    "\n",
    "# Arbol de Decision\n",
    "max_depth_values = [None, 6, 20];\n",
    "min_samples_split_values = [2, 5, 20];\n",
    "min_samples_leaf_values = [1, 5, 20];\n",
    "max_features_values = [None, 1, 2];\n",
    "\n",
    "# Random Forest\n",
    "ntrees_values = [10, 100, 1000];\n",
    "\n",
    "# Xgboost\n",
    "nrounds_values = [10, 100]\n",
    "eta_values = [0.3, 0.99]\n",
    "gamma_values = [0, 1]\n",
    "max_depth_values = [6, 20]\n",
    "min_child_weight_values = [1, 20]\n",
    "subsample_values = [0.1, 1]\n",
    "colsample_bytree_values = [0.1, 1]\n",
    "num_parallel_tree_values = [1, 20]\n",
    "lambda_values = [0, 1]\n",
    "alpha_values = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_values = [{'model': 'logistic regression',\n",
    "                  'regularization': regularization_values,\n",
    "                 'penalty': penalty_values},\n",
    "                 {'model': 'svm',\n",
    "                  'C': C_values,\n",
    "                 'gamma_kernel': gamma_kernel_values},\n",
    "                 {'model': 'decision tree',\n",
    "                 'max_depth': max_depth_values,\n",
    "                 'min_samples_split': min_samples_split_values,\n",
    "                 'min_samples_leaf': min_samples_leaf_values,\n",
    "                 'max_features': max_features_values},\n",
    "                 {'model': 'random forest',\n",
    "                  'n_trees': ntrees_values,\n",
    "                 'max_depth': max_depth_values,\n",
    "                 'min_samples_split': min_samples_split_values,\n",
    "                 'min_samples_leaf': min_samples_leaf_values,\n",
    "                 'max_features': max_features_values},\n",
    "                 {'model': 'xgboost',\n",
    "                  'nrounds': nrounds_values,\n",
    "                  'eta': eta_values,\n",
    "                 'gamma': gamma_values,\n",
    "                 'max_depth': max_depth_values,\n",
    "                 'min_child_weight': min_child_weight_values,\n",
    "                 'subsample': subsample_values,\n",
    "                 'colsample_bytree': colsample_bytree_values,\n",
    "                 'num_parallel_tree': num_parallel_tree_values,\n",
    "                 'lambda': lambda_values,\n",
    "                 'alpha': alpha_values}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_values = [{'model': 'svm',\n",
    "                  'C': C_values,\n",
    "                 'gamma_kernel': gamma_kernel_values},\n",
    "                 {'model': 'decision tree',\n",
    "                 'max_depth': max_depth_values,\n",
    "                 'min_samples_split': min_samples_split_values,\n",
    "                 'min_samples_leaf': min_samples_leaf_values,\n",
    "                 'max_features': max_features_values},\n",
    "                 {'model': 'random forest',\n",
    "                  'n_trees': ntrees_values,\n",
    "                 'max_depth': max_depth_values,\n",
    "                 'min_samples_split': min_samples_split_values,\n",
    "                 'min_samples_leaf': min_samples_leaf_values,\n",
    "                 'max_features': max_features_values},\n",
    "                 {'model': 'xgboost',\n",
    "                  'nrounds': nrounds_values,\n",
    "                  'eta': eta_values,\n",
    "                 'gamma': gamma_values,\n",
    "                 'max_depth': max_depth_values,\n",
    "                 'min_child_weight': min_child_weight_values,\n",
    "                 'subsample': subsample_values,\n",
    "                 'colsample_bytree': colsample_bytree_values,\n",
    "                 'num_parallel_tree': num_parallel_tree_values,\n",
    "                 'lambda': lambda_values,\n",
    "                 'alpha': alpha_values}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iteraciones = 0\n",
    "for params in params_values:\n",
    "    if params['model'] == 'logistic regression':\n",
    "        n = len(params['regularization'])*len(params['penalty'])\n",
    "    elif params['model'] == 'svm':\n",
    "        n = len(params['C'])*len(params['gamma_kernel'])\n",
    "    elif params['model'] == 'decision tree':\n",
    "        n = len(params['max_depth'])*len(params['min_samples_split'])*len(params['min_samples_leaf'])*len(params['max_features'])\n",
    "    elif params['model'] == 'random forest':\n",
    "        n = len(params['n_trees'])*len(params['max_depth'])*len(params['min_samples_split'])*len(params['min_samples_leaf'])*len(params['max_features'])\n",
    "    elif params['model'] == 'xgboost':\n",
    "        n = len(params['nrounds'])*len(params['eta'])*len(params['gamma'])*len(params['max_depth'])*len(params['min_child_weight'])*len(params['subsample'])*len(params['colsample_bytree'])*len(params['num_parallel_tree'])*len(params['lambda'])*len(params['alpha'])\n",
    "    total_iteraciones = total_iteraciones + n;\n",
    "    print(str(n)+ ' iteraciones de ' + str(params['model']))\n",
    "print(str(total_iteraciones)+ ' iteraciones en total')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = pd.DataFrame();\n",
    "num_iter = 0\n",
    "for params in params_values:\n",
    "    \n",
    "    # Logistic Regression\n",
    "    if params['model'] == 'logistic regression':\n",
    "        for regularization in params['regularization']:\n",
    "            for penalty in params['penalty']:  \n",
    "                \n",
    "                # Actualizar contador\n",
    "                num_iter += 1; \n",
    "                \n",
    "                # print control iteracion modelo\n",
    "                print('Inicio de iteracion ' + str(num_iter) + \n",
    "                      '. Regularizacion = ' + str(regularization) + \n",
    "                      ', Lambda = '  + str(penalty) +\n",
    "                      '\\n')\n",
    "                \n",
    "                # Entrenar modelo\n",
    "                if regularization == 'l1':\n",
    "                    model = LogisticRegression(penalty = regularization, solver = 'liblinear', C = penalty, random_state = random_state)\n",
    "                else:\n",
    "                    model = LogisticRegression(penalty = regularization,solver = 'lbfgs', C = penalty, random_state = random_state)\n",
    "               \n",
    "                model.fit(X_train, np.array(y_train))\n",
    "\n",
    "                # Generar predicciones\n",
    "                pred_train_p = model.predict_proba(X_train)\n",
    "                pred_val_p = model.predict_proba(X_val)\n",
    "\n",
    "                # Calcular métricas de evaluación\n",
    "                auc_train = metric(y_train, pred_train_p[:, 1])\n",
    "                auc_val = metric(y_val, pred_val_p[:, 1])                                            \n",
    "\n",
    "                print('Fin de iteracion ' + str(num_iter) + \n",
    "                     '. Regularizacion = ' + str(regularization) + \n",
    "                      ', Lambda = '  + str(penalty) +\n",
    "                      '. AUC train = '  + str(auc_train) + \n",
    "                      ' -  AUC val = '  + str(auc_val)  + \n",
    "                      '\\n')\n",
    "                grid_results = grid_results.append(pd.DataFrame(data={'model':'Logistic Regression',\n",
    "                                                                      'params': [{'regularization':[regularization],\n",
    "                                                                                  'penalty':[penalty]}],\n",
    "                                                                      'auc_train':[auc_train],\n",
    "                                                                      'auc_val':[auc_val]},\n",
    "                                                               columns=['model','params', 'auc_train', 'auc_val']), \n",
    "                                                   ignore_index=True)\n",
    "                \n",
    "     \n",
    "    # SVM\n",
    "    if params['model'] == 'svm':\n",
    "        for C in params['C']:\n",
    "            for gamma_kernel in params['gamma_kernel']:  \n",
    "                \n",
    "                # Actualizar contador\n",
    "                num_iter += 1; \n",
    "                \n",
    "                # print control iteracion modelo\n",
    "                print('Inicio de iteracion ' + str(num_iter) + \n",
    "                      '. C = ' + str(C) + \n",
    "                      ', gamma = '  + str(gamma_kernel) +\n",
    "                      '\\n')\n",
    "                \n",
    "                # Entrenar modelo               \n",
    "                model = SVC(C = C, gamma = gamma_kernel, probability = True, random_state = random_state)\n",
    "               \n",
    "                model.fit(X_train, np.array(y_train))\n",
    "\n",
    "                # Generar predicciones\n",
    "                pred_train_p = model.predict_proba(X_train)\n",
    "                pred_val_p = model.predict_proba(X_val)\n",
    "\n",
    "                # Calcular métricas de evaluación\n",
    "                auc_train = metric(y_train, pred_train_p[:, 1])\n",
    "                auc_val = metric(y_val, pred_val_p[:, 1])                                            \n",
    "\n",
    "                print('Fin de iteracion ' + str(num_iter) + \n",
    "                     '. C = ' + str(C) + \n",
    "                      ', gamma = '  + str(gamma_kernel) +\n",
    "                      '. AUC train = '  + str(auc_train) + \n",
    "                      ' -  AUC val = '  + str(auc_val)  + \n",
    "                      '\\n')\n",
    "                grid_results = grid_results.append(pd.DataFrame(data={'model':'SVM',\n",
    "                                                                     'params': [{'C':[C],\n",
    "                                                                              'gamma_kernel':[gamma_kernel]}],\n",
    "                                                                      'auc_train':[auc_train],\n",
    "                                                                      'auc_val':[auc_val]},\n",
    "                                                               columns=['model','params', 'auc_train', 'auc_val']), \n",
    "                                                   ignore_index=True)\n",
    "                \n",
    "    # Decision Tree\n",
    "    if params['model'] == 'decision tree':\n",
    "        for max_depth in params['max_depth']:\n",
    "            for min_samples_split in params['min_samples_split']:  \n",
    "                for min_samples_leaf in params['min_samples_leaf']:  \n",
    "                    for max_features in params['max_features']:  \n",
    "                \n",
    "                        # Actualizar contador\n",
    "                        num_iter += 1; \n",
    "\n",
    "                        # print control iteracion modelo\n",
    "                        print('Inicio de iteracion ' + str(num_iter) + \n",
    "                              '. max_depth = ' + str(max_depth) + \n",
    "                              ', min_samples_split = '  + str(min_samples_split) +\n",
    "                              ', min_samples_leaf = '  + str(min_samples_leaf) +\n",
    "                              ', max_features = '  + str(max_features) +\n",
    "                              '\\n')\n",
    "\n",
    "                        # Entrenar modelo               \n",
    "                        model = DecisionTreeClassifier(max_depth = max_depth,\n",
    "                                                      min_samples_split = min_samples_split,\n",
    "                                                      min_samples_leaf = min_samples_leaf,\n",
    "                                                      max_features = max_features, random_state = random_state)\n",
    "\n",
    "                        model.fit(X_train, np.array(y_train))\n",
    "\n",
    "                        # Generar predicciones\n",
    "                        pred_train_p = model.predict_proba(X_train)\n",
    "                        pred_val_p = model.predict_proba(X_val)\n",
    "\n",
    "                        # Calcular métricas de evaluación\n",
    "                        auc_train = metric(y_train, pred_train_p[:, 1])\n",
    "                        auc_val = metric(y_val, pred_val_p[:, 1])                                            \n",
    "\n",
    "                        print('Fin de iteracion ' + str(num_iter) + \n",
    "                             '. max_depth = ' + str(max_depth) + \n",
    "                              ', min_samples_split = '  + str(min_samples_split) +\n",
    "                              ', min_samples_leaf = '  + str(min_samples_leaf) +\n",
    "                              ', max_features = '  + str(max_features) +\n",
    "                              '. AUC train = '  + str(auc_train) + \n",
    "                              ' -  AUC val = '  + str(auc_val)  + \n",
    "                              '\\n')\n",
    "                        grid_results = grid_results.append(pd.DataFrame(data={'model':'decision tree',\n",
    "                                                                              'params': [{'max_depth':[max_depth],\n",
    "                                                                                          'min_samples_split':[min_samples_split],\n",
    "                                                                                          'min_samples_leaf':[min_samples_leaf],\n",
    "                                                                                          'max_features':[max_features]}],\n",
    "                                                                      'auc_train':[auc_train],\n",
    "                                                                      'auc_val':[auc_val]},\n",
    "                                                                       columns=['model','params', 'auc_train', 'auc_val']), \n",
    "                                                           ignore_index=True)  \n",
    "                        \n",
    "    \n",
    "    # Random Forest\n",
    "    if params['model'] == 'random forest':\n",
    "        for n_trees in params['n_trees']:\n",
    "            for max_depth in params['max_depth']:\n",
    "                for min_samples_split in params['min_samples_split']:  \n",
    "                    for min_samples_leaf in params['min_samples_leaf']:  \n",
    "                        for max_features in params['max_features']:  \n",
    "                \n",
    "                            # Actualizar contador\n",
    "                            num_iter += 1; \n",
    "\n",
    "                            # print control iteracion modelo\n",
    "                            print('Inicio de iteracion ' + str(num_iter) + \n",
    "                                  '. n_trees = ' + str(n_trees) + \n",
    "                                  ', max_depth = ' + str(max_depth) + \n",
    "                                  ', min_samples_split = '  + str(min_samples_split) +\n",
    "                                  ', min_samples_leaf = '  + str(min_samples_leaf) +\n",
    "                                  ', max_features = '  + str(max_features) +\n",
    "                                  '\\n')\n",
    "\n",
    "                            # Entrenar modelo               \n",
    "                            model = RandomForestClassifier(n_estimators = n_trees,\n",
    "                                                          max_depth = max_depth,\n",
    "                                                          min_samples_split = min_samples_split,\n",
    "                                                          min_samples_leaf = min_samples_leaf,\n",
    "                                                          max_features = max_features, random_state = random_state)\n",
    "\n",
    "                            model.fit(X_train, np.array(y_train))\n",
    "\n",
    "                            # Generar predicciones\n",
    "                            pred_train_p = model.predict_proba(X_train)\n",
    "                            pred_val_p = model.predict_proba(X_val)\n",
    "\n",
    "                            # Calcular métricas de evaluación\n",
    "                            auc_train = metric(y_train, pred_train_p[:, 1])\n",
    "                            auc_val = metric(y_val, pred_val_p[:, 1])                                            \n",
    "\n",
    "                            print('Fin de iteracion ' + str(num_iter) + \n",
    "                                 '. n_trees = ' + str(n_trees) + \n",
    "                                  ', max_depth = ' + str(max_depth) + \n",
    "                                  ', min_samples_split = '  + str(min_samples_split) +\n",
    "                                  ', min_samples_leaf = '  + str(min_samples_leaf) +\n",
    "                                  ', max_features = '  + str(max_features) +\n",
    "                                  '. AUC train = '  + str(auc_train) + \n",
    "                                  ' -  AUC val = '  + str(auc_val)  + \n",
    "                                  '\\n')\n",
    "                            grid_results = grid_results.append(pd.DataFrame(data={'model':'random forest',\n",
    "                                                                                  'params': [{'n_trees':[n_trees],\n",
    "                                                                                              'max_depth':[max_depth],\n",
    "                                                                                              'min_samples_split':[min_samples_split],\n",
    "                                                                                              'min_samples_leaf':[min_samples_leaf],\n",
    "                                                                                              'max_features':[max_features]}],\n",
    "                                                                          'auc_train':[auc_train],\n",
    "                                                                          'auc_val':[auc_val]},\n",
    "                                                                           columns=['model','params', 'auc_train', 'auc_val']), \n",
    "                                                               ignore_index=True)  \n",
    "    \n",
    "    # XGBOOST\n",
    "    if params['model'] == 'xgboost':\n",
    "         for nrounds in params['nrounds']:\n",
    "            for eta in params['eta']:\n",
    "                for gamma in params['gamma']:\n",
    "                    for max_depth in params['max_depth']:\n",
    "                        for min_child_weight in params['min_child_weight']:\n",
    "                            for subsample in params['subsample']:\n",
    "                                for colsample_bytree in params['colsample_bytree']:\n",
    "                                    for num_parallel_tree in params['num_parallel_tree']:\n",
    "                                        for lamda in params['lambda']:\n",
    "                                            for alpha in params['alpha']:\n",
    "                                            \n",
    "                                                # Actualizar contador\n",
    "                                                num_iter += 1; \n",
    "\n",
    "                                                # print control iteracion modelo\n",
    "                                                print('Inicio de iteracion ' + str(num_iter) + \n",
    "                                                      '. Parametro nrounds = ' + str(nrounds) + \n",
    "                                                      ', parametro eta = '  + str(eta) +\n",
    "                                                      ', parametro gamma = '  + str(gamma) +\n",
    "                                                      ', parametro max_depth = '  + str(max_depth) +\n",
    "                                                      ', parametro min_child_weight = '  + str(min_child_weight) +\n",
    "                                                      ', parametro subsample = '  + str(subsample) +\n",
    "                                                      ', parametro colsample_bytree = '  + str(colsample_bytree) +\n",
    "                                                      ', parametro num_parallel_tree = '  + str(num_parallel_tree) +\n",
    "                                                      ', parametro lambda = '  + str(lamda) +\n",
    "                                                      ', parametro alpha = '  + str(alpha) + \n",
    "                                                      '\\n')\n",
    "                                                # Entrenar modelo\n",
    "                                                model = XGBClassifier(nthread = nthread, \n",
    "                                                                      scale_pos_weight = scale_pos_weight,\n",
    "                                                                      random_state = random_state,\n",
    "                                                                      n_estimators = nrounds,\n",
    "                                                                      learning_rate = eta, \n",
    "                                                                      gamma = gamma,\n",
    "                                                                      max_depth = max_depth,\n",
    "                                                                      min_child_weight = min_child_weight ,\n",
    "                                                                      subsample = subsample,\n",
    "                                                                      colsample_bytree = colsample_bytree,\n",
    "                                                                      num_parallel_tree = num_parallel_tree,\n",
    "                                                                      reg_lambda = lamda,\n",
    "                                                                      reg_alpha = alpha)\n",
    "                                                model.fit(X_train, np.array(y_train))\n",
    "\n",
    "                                                # Generar predicciones\n",
    "                                                pred_train_p = model.predict_proba(X_train)\n",
    "                                                pred_val_p = model.predict_proba(X_val)\n",
    "\n",
    "                                                # Calcular métricas de evaluación\n",
    "                                                auc_train = metric(y_train, pred_train_p[:, 1])\n",
    "                                                auc_val = metric(y_val, pred_val_p[:, 1])                                            \n",
    "\n",
    "                                                print('Fin de iteracion ' + str(num_iter) + \n",
    "                                                      '. Parametro nrounds = ' + str(nrounds) + \n",
    "                                                      ', parametro eta = ' + str(eta) + \n",
    "                                                      ', parametro gamma = '  + str(gamma) +\n",
    "                                                      ', parametro max_depth = '  + str(max_depth) +\n",
    "                                                      ', parametro min_child_weight = '  + str(min_child_weight) +\n",
    "                                                      ', parametro subsample = '  + str(subsample) +\n",
    "                                                      ', parametro colsample_bytree = '  + str(colsample_bytree) +\n",
    "                                                      ', parametro num_parallel_tree = '  + str(num_parallel_tree) +\n",
    "                                                      ', parametro lambda = '  + str(lamda) +\n",
    "                                                      ', parametro alpha = '  + str(alpha) + \n",
    "                                                      '. AUC train = '  + str(auc_train) + \n",
    "                                                      ' -  AUC val = '  + str(auc_val)  + \n",
    "                                                      '\\n')\n",
    "                                                grid_results = grid_results.append(pd.DataFrame(data={'model':'xgboost',\n",
    "                                                                                              'params': [{'nrounds':[nrounds],\n",
    "                                                                                              'eta':[eta],\n",
    "                                                                                              'gamma':[gamma],\n",
    "                                                                                              'max_depth':[max_depth],\n",
    "                                                                                              'min_child_weight':[min_child_weight],\n",
    "                                                                                              'subsample':[subsample],\n",
    "                                                                                              'colsample_bytree':[colsample_bytree],\n",
    "                                                                                              'num_parallel_tree':[num_parallel_tree],\n",
    "                                                                                              'lamda':[lamda],\n",
    "                                                                                              'alpha':[alpha]}],\n",
    "                                                                                              'auc_train':[auc_train],\n",
    "                                                                                              'auc_val':[auc_val]},\n",
    "                                                                                               columns=['model', 'params', 'auc_train', 'auc_val']), \n",
    "                                                                                   ignore_index=True)\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results[grid_results.model=='random forest'<+.groupby(['model'], sort=False)['auc_val'].max().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_results[grid_results.model=='random forest'].iloc[grid_results['auc_val'].idxmax()]\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data size = ' + str(X_train.shape))\n",
    "print('Train target size = ' + str(y_train.shape))\n",
    "print('Validation data size = ' + str(X_val.shape))\n",
    "print('Validation target size = ' + str(y_val.shape))\n",
    "\n",
    "# Combinar train y validación\n",
    "X_train = pd.concat((X_train,X_val), axis = 0)\n",
    "y_train = np.concatenate((y_train, y_val), axis = 0)\n",
    "\n",
    "del X_val, y_val\n",
    "\n",
    "print('Train data size = ' + str(X_train.shape))\n",
    "print('Train target size = ' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "if best_params['model'] == 'logistic regression':       \n",
    "\n",
    "    # Entrenar modelo\n",
    "    if best_params['params']['regularization'] == 'l1':\n",
    "        model = LogisticRegression(penalty = best_params['params']['regularization'][0], solver = 'liblinear', C = best_params['params']['penalty'][0], random_state = random_state)\n",
    "    else:\n",
    "        model = LogisticRegression(penalty = best_params['params']['regularization'][0],solver = 'lbfgs', C = best_params['params']['penalty'][0], random_state = random_state)\n",
    "\n",
    "\n",
    "# SVM\n",
    "elif best_params['model'] == 'SVM':\n",
    "\n",
    "    model = SVC(C = best_params['params']['C'][0], gamma = best_params['params']['gamma_kernel'][0], probability = True, \n",
    "                random_state = random_state)             \n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "elif best_params['model'] == 'decision tree':\n",
    "    model = DecisionTreeClassifier(max_depth = int(best_params['params']['max_depth'][0]),\n",
    "                                                  min_samples_split = int(best_params['params']['min_samples_split'][0]),\n",
    "                                                  min_samples_leaf = int(best_params['params']['min_samples_leaf'][0]),\n",
    "                                                  max_features = int(best_params['params']['max_features'][0]), \n",
    "                                   random_state = random_state)\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "elif best_params['model'] == 'random forest':\n",
    "    model = RandomForestClassifier(n_estimators = int(best_params['params']['n_trees'][0]),\n",
    "                                                      max_depth = int(best_params['params']['max_depth'][0]),\n",
    "                                                      min_samples_split = int(best_params['params']['min_samples_split'][0]),\n",
    "                                                      min_samples_leaf = int(best_params['params']['min_samples_leaf'][0]),\n",
    "                                                      max_features = int(best_params['params']['max_features'][0]), \n",
    "                                                      random_state = random_state)\n",
    "\n",
    "# XGBOOST\n",
    "elif best_params['model'] == 'xgboost':\n",
    "    model = XGBClassifier(nthread = nthread, \n",
    "                                                              scale_pos_weight = scale_pos_weight,\n",
    "                                                              random_state = random_state,\n",
    "                                                              n_estimators = int(best_params['params']['nrounds'][0]), \n",
    "                                                              learning_rate = best_params['params']['eta'][0], \n",
    "                                                              gamma = best_params['params']['gamma'][0],\n",
    "                                                              max_depth = int(best_params['params']['max_depth'][0]),\n",
    "                                                              min_child_weight = best_params['params']['min_child_weight'][0],\n",
    "                                                              subsample = best_params['params']['subsample'][0],\n",
    "                                                              colsample_bytree = best_params['params']['colsample_bytree'][0],\n",
    "                                                              num_parallel_tree  = int(best_params['params']['num_parallel_tree'][0]),\n",
    "                                                              reg_lambda = best_params['params']['lamda'][0],\n",
    "                                                              reg_alpha = best_params['params']['alpha'][0])\n",
    "\n",
    "# Entrenar modelo\n",
    "model.fit(X_train, np.array(y_train))\n",
    "\n",
    "# Generar predicciones\n",
    "pred_train_p = model.predict_proba(X_train)\n",
    "pred_test_p = model.predict_proba(X_test)\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "auc_train = metric(y_train, pred_train_p[:, 1])\n",
    "auc_test = metric(y_test, pred_test_p[:, 1]) \n",
    "\n",
    "results = pd.DataFrame()\n",
    "results = results.append(pd.DataFrame(data={'model':best_params['model'],'auc_train':[auc_train],'auc_test':[auc_test]}, columns=['model',  'auc_train', 'auc_test']), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
