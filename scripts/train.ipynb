{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Generic\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Images\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import talos as ta\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, roc_curve\n",
    "\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import print_summary\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.densenet import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_train.reshape((y_train.shape[0],1))\n",
    "y_probas = np.hstack((1-pred_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc_curve(y_true = y_true, y_probas = [1-pred_train, pred_train])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_train, tpr_train, threshold_train = roc_curve(y_train, pred_train)\n",
    "roc_auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "fpr_val, tpr_val, threshold_val = roc_curve(y_val, pred_val)\n",
    "roc_auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "fpr_test, tpr_test, threshold_test = roc_curve(y_test, pred_test)\n",
    "roc_auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'r', label = 'AUC = %0.2f' % roc_auc_train)\n",
    "plt.plot(fpr_val, tpr_val, 'g', label = 'AUC = %0.2f' % roc_auc_val)\n",
    "plt.plot(fpr_test, tpr_test, 'b', label = 'AUC = %0.2f' % roc_auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_true = y_train, y_score = pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chexNet weights\n",
    "# https://github.com/brucechou1983/CheXNet-Keras\n",
    "chexnet_weights = 'chexnet/best_weights.h5'\n",
    "\n",
    "def chexnet_preprocess_input(value):\n",
    "    return preprocess_input(value)\n",
    "\n",
    "\n",
    "def get_chexnet_model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    base_weights = 'imagenet'\n",
    "\n",
    "    # create the base pre-trained model\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        input_tensor=img_input,\n",
    "        input_shape=input_shape,\n",
    "        weights=base_weights,\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    x = base_model.output\n",
    "    # add a logistic layer -- let's say we have 14 classes\n",
    "    predictions = Dense(\n",
    "        14,\n",
    "        activation='sigmoid',\n",
    "        name='predictions')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=img_input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # load chexnet weights\n",
    "    model.load_weights(chexnet_weights)\n",
    "\n",
    "    # return model\n",
    "    return base_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### funciones\n",
    "\n",
    "def get_class_weight(csv_file_path, target_class):\n",
    "    df = pd.read_csv(csv_file_path, sep=';')\n",
    "    total_counts = df.shape[0]\n",
    "    class_weight = []\n",
    "\n",
    "    ratio_pos = df.loc[(df[target_class] == 'Y')].shape[0] / total_counts\n",
    "    ratio_neg = df.loc[(df[target_class] == 'N')].shape[0] / total_counts\n",
    "    class_weight = np.array((ratio_pos, ratio_neg))\n",
    "        \n",
    "    return class_weight\n",
    "\n",
    "#def auc(y_true, y_pred):\n",
    "#    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "#    K.get_session().run(tf.local_variables_initializer())\n",
    "#    return auc\n",
    "\n",
    "# define roc_callback, inspired by https://github.com/keras-team/keras/issues/6050#issuecomment-329996505\n",
    "def auc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.metrics.auc(y_true, y_pred)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "    return value\n",
    "\n",
    "#tf.keras.metrics.AUC(\n",
    "#    num_thresholds=200, curve='ROC', summation_method='interpolation', name=None,\n",
    "#    dtype=None, thresholds=None, multi_label=False, label_weights=None\n",
    "#)\n",
    "\n",
    "def get_model():\n",
    "    # get base model, model\n",
    "    base_model, chexnet_model = get_chexnet_model()\n",
    "    # print a model summary\n",
    "    # print_summary(base_model)\n",
    "\n",
    "    x = base_model.output\n",
    "    # Dropout layer\n",
    "    x = Dropout(0.2)(x)\n",
    "    # one more layer (relu)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    # Dropout layer\n",
    "    x = Dropout(0.2)(x)\n",
    "    # add a logistic layer -- let's say we have 6 classes\n",
    "    predictions = Dense(\n",
    "        1,\n",
    "        activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will use\n",
    "    model = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=predictions,\n",
    "    )\n",
    "\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all base_model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # initiate an Adam optimizer\n",
    "    opt = Adam(\n",
    "        lr=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        decay=0.0,\n",
    "        amsgrad=False\n",
    "    )\n",
    "\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy', auc])\n",
    "\n",
    "    return base_model, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_split = [0.8, 0.2]\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "### Hyperparameters\n",
    "batch_size = 16\n",
    "epochs = 10000\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case_features = pd.read_csv(os.path.join('../data/clean_data.csv'), sep=';');\n",
    "new_case_features = pd.read_csv(os.path.join('../data/new_clean_data.csv'), sep=';');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 'survival'\n",
    "np.random.seed(23)\n",
    "\n",
    "### Train and val\n",
    "# Split patient ids\n",
    "positive = case_features[case_features.survival.values == 'Y']\n",
    "patient_ids = positive.patientid.unique()\n",
    "train_ids = np.random.choice(patient_ids, size = math.floor(perc_split[0]*len(patient_ids)), replace = False)\n",
    "test_ids = patient_ids[~np.isin(patient_ids, train_ids)];\n",
    "\n",
    "negative = case_features[case_features.survival.values == 'N']\n",
    "patient_ids = negative.patientid.unique()\n",
    "train_ids = np.append(train_ids, np.random.choice(patient_ids, size = math.floor(perc_split[0]*len(patient_ids)), replace = False))\n",
    "test_ids = np.append(test_ids, patient_ids[~np.isin(patient_ids, train_ids)]);\n",
    "\n",
    "# Split dataset based on patient ids\n",
    "case_features_train = case_features[case_features.patientid.isin(train_ids)]\n",
    "case_features_val = case_features[case_features.patientid.isin(test_ids)]\n",
    "\n",
    "# Selección del patrón de datos X y del target y\n",
    "ytrain = case_features_train[target_class]\n",
    "del case_features_train[target_class]\n",
    "Xtrain = case_features_train\n",
    "\n",
    "yval = case_features_val[target_class]\n",
    "del case_features_val[target_class]\n",
    "Xval = case_features_val\n",
    "\n",
    "\n",
    "### Test\n",
    "ytest = new_case_features[target_class]\n",
    "del new_case_features[target_class]\n",
    "Xtest = new_case_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(Xtrain.shape[0]):\n",
    "    # print('%0.2f%%' % float(100*i/Xtrain.shape[0]))\n",
    "    image_path = os.path.join('../data/', 'images', Xtrain.iloc[i].filename)\n",
    "    imagen = Image.open(image_path)\n",
    "    imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_train.append(imagen)\n",
    "\n",
    "X_train = np.stack(X_train, axis = 0)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "for i in range(Xval.shape[0]):\n",
    "    # print('%0.2f%%' % float(100*i/Xval.shape[0]))\n",
    "    image_path = os.path.join('../data/', 'images', Xval.iloc[i].filename)\n",
    "    imagen = Image.open(image_path)\n",
    "    imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_val.append(imagen)\n",
    "\n",
    "X_val = np.stack(X_val, axis = 0)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(Xtest.shape[0]):\n",
    "    # print('%0.2f%%' % float(100*i/Xtest.shape[0]))\n",
    "    image_path = os.path.join('../data/', 'new_images', Xtest.iloc[i].filename + '.jpeg')\n",
    "    imagen = Image.open(image_path)\n",
    "    imagen = np.asarray(imagen.convert(\"RGB\"))\n",
    "    imagen = resize(imagen,  input_shape)\n",
    "    X_test.append(imagen)\n",
    "\n",
    "X_test = np.stack(X_test, axis = 0)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain[ytrain=='Y'] = 0\n",
    "ytrain[ytrain=='N'] = 1\n",
    "y_train = np.array(ytrain, dtype = np.int64)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yval[yval=='Y'] = 0\n",
    "yval[yval=='N'] = 1\n",
    "y_val = np.array(yval, dtype = np.int64)\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest[ytest=='Y'] = 0\n",
    "ytest[ytest=='N'] = 1\n",
    "y_test = np.array(ytest, dtype = np.int64)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pos = np.count_nonzero(y_train == 0) / len(y_train)\n",
    "ratio_neg = np.count_nonzero(y_train == 1) / len(y_train)\n",
    "class_weight_train = np.array((ratio_pos, ratio_neg))\n",
    "print(class_weight_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True, \n",
    "                             rotation_range=90)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "    os.getcwd(),\n",
    "    '../saved_models'\n",
    ")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# This callback saves the weights of the model after each epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '../saved_models/weights.epoch_{epoch:02d}.hdf5',\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./Graph', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard_augmented = TensorBoard(\n",
    "    log_dir='./Graph_augmented', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "\n",
    "my_callbacks = EarlyStopping(monitor='val_auc', patience=100, verbose=1, mode='max')\n",
    "my_callbacks_augmented = EarlyStopping(monitor='val_auc', patience=100, verbose=1, mode='max')\n",
    "\n",
    "callbacks_list = [tensorboard, my_callbacks]\n",
    "callbacks_list_augmented = [tensorboard_augmented, my_callbacks_augmented]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed\n",
    "base_model, model = get_model()\n",
    "\n",
    "# Augmented\n",
    "base_model_augmented, model_augmented = get_model()\n",
    "\n",
    "# Show layers\n",
    "#print_summary(model)\n",
    "#print_summary(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ENTRENAMIENTO\n",
    "history = model.fit(X_train,y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          batch_size=batch_size, \n",
    "          nb_epoch=epochs,\n",
    "          class_weight=class_weight_train,\n",
    "          callbacks = callbacks_list,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "    os.getcwd(),\n",
    "    '../model_results'\n",
    ")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model.save('../model_results/model.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_val = model.predict(X_val)\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "print('AUC train = %s - AUC val = %s - AUC test = %s' % (str(auc_train), str(auc_val), str(auc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_train = (pred_train >= 0.5).astype(int)\n",
    "y_labels_val = (pred_val >= 0.5).astype(int)\n",
    "y_labels_test = (pred_test >= 0.5).astype(int)\n",
    "cm_train = confusion_matrix(y_pred = y_labels_train, y_true = y_train)\n",
    "cm_val = confusion_matrix(y_pred = y_labels_val, y_true = y_val)\n",
    "cm_test = confusion_matrix(y_pred = y_labels_test, y_true = y_test)\n",
    "print(cm_train)\n",
    "print(cm_val)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(os.path.join('../predictions', 'X_train.csv'))\n",
    "np.savetxt(os.path.join('../predictions', 'predictions_train.csv'), pred_train, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions', 'y_train.csv'), y_train, delimiter=\";\")\n",
    "X_val.to_csv(os.path.join('../predictions', 'X_val.csv'))\n",
    "np.savetxt(os.path.join('../predictions', 'predictions_val.csv'), pred_val, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions', 'y_val.csv'), y_val, delimiter=\";\")\n",
    "X_test.to_csv(os.path.join('../predictions', 'X_test.csv'))\n",
    "np.savetxt(os.path.join('../predictions', 'predictions_test.csv'),pred_test, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions', 'y_test.csv'), y_test, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ENTRENAMIENTO\n",
    "model_augmented.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                     validation_data=datagen.flow(X_val, y_val),\n",
    "                     steps_per_epoch=len(X_train) / batch_size, \n",
    "                     epochs=epochs,\n",
    "                     class_weight=class_weight_train,\n",
    "                     callbacks = callbacks_list_augmented,       \n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_augmented.save('../model_results/model_augmented.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model_augmented.predict(X_train)\n",
    "pred_val = model_augmented.predict(X_val)\n",
    "pred_test = model_augmented.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "print('AUC train = %s - AUC val = %s - AUC test = %s' % (str(auc_train), str(auc_val), str(auc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels_train = (pred_train >= 0.5).astype(int)\n",
    "y_labels_val = (pred_val >= 0.5).astype(int)\n",
    "y_labels_test = (pred_test >= 0.5).astype(int)\n",
    "cm_train = confusion_matrix(y_pred = y_labels_train, y_true = y_train)\n",
    "cm_val = confusion_matrix(y_pred = y_labels_val, y_true = y_val)\n",
    "cm_test = confusion_matrix(y_pred = y_labels_test, y_true = y_test)\n",
    "print(cm_train)\n",
    "print(cm_val)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.to_csv(os.path.join('../predictions', 'X_train_augmented.csv'))\n",
    "np.savetxt(os.path.join('../predictions', 'predictions_train_augmented.csv'), pred_train, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions', 'y_train_augmented.csv'), y_train, delimiter=\";\")\n",
    "Xval.to_csv(os.path.join('../predictions', 'X_val_augmented.csv'))\n",
    "np.savetxt(os.path.join('../predictions', 'predictions_val_augmented.csv'), pred_val, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions', 'y_val_augmented.csv'), y_val, delimiter=\";\")\n",
    "Xtest.to_csv(os.path.join('../predictions', 'X_test_augmented.csv'))\n",
    "np.savetxt(os.path.join('../predictions', 'predictions_test_augmented.csv'),pred_test, delimiter=\";\")\n",
    "np.savetxt(os.path.join('../predictions', 'y_test_augmented.csv'), y_test, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
